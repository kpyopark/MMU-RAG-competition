{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.pipeline import run_rag_static\n",
    "from local_test import load_validation_data\n",
    "import random\n",
    "\n",
    "data = load_validation_data(\"t2t_val.jsonl\")\n",
    "idx = random.randint(0, len(data) - 1)\n",
    "item = data[idx]\n",
    "query = item[\"query\"]\n",
    "iid = item[\"iid\"]\n",
    "print(item)\n",
    "# report = run_rag_static(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c44ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-07 16:36:29 [utils.py:233] non-default args: {'task': 'score', 'disable_log_stats': True, 'enforce_eager': True, 'model': 'BAAI/bge-reranker-v2-m3'}\n",
      "INFO 10-07 16:36:29 [model.py:547] Resolved architecture: XLMRobertaForSequenceClassification\n",
      "INFO 10-07 16:36:29 [model.py:1730] Downcasting torch.float32 to torch.float16.\n",
      "INFO 10-07 16:36:29 [model.py:1510] Using max model len 8192\n",
      "INFO 10-07 16:36:29 [arg_utils.py:1575] (Disabling) chunked prefill by default\n",
      "INFO 10-07 16:36:29 [arg_utils.py:1578] (Disabling) prefix caching by default\n",
      "INFO 10-07 16:36:29 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 10-07 16:36:29 [__init__.py:440] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m INFO 10-07 16:36:31 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m INFO 10-07 16:36:31 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='BAAI/bge-reranker-v2-m3', speculative_config=None, tokenizer='BAAI/bge-reranker-v2-m3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=BAAI/bge-reranker-v2-m3, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=PoolerConfig(pooling_type='CLS', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, activation=None, logit_bias=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m INFO 10-07 16:36:34 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 187, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ERROR 10-07 16:36:35 [core.py:708] ValueError: Free memory on device (2.45/22.28 GiB) on startup is less than desired GPU memory utilization (0.9, 20.05 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/system/conda/miniconda3/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/system/conda/miniconda3/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m   File \"/teamspace/studios/this_studio/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 187, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=251602)\u001b[0;0m ValueError: Free memory on device (2.45/22.28 GiB) on startup is less than desired GPU memory utilization (0.9, 20.05 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m texts_2 = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe capital of Brazil is Brasilia.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe capital of France is Paris.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m ]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create an LLM.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# You should pass task=\"score\" for cross-encoder models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBAAI/bge-reranker-v2-m3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Generate scores. The output is a list of ScoringRequestOutputs.\u001b[39;00m\n\u001b[32m     20\u001b[39m outputs = model.score(text_1, texts_2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:297\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    174\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: Optional[StatLoggerManager] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:602\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    601\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    610\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:448\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    445\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/system/conda/miniconda3/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:732\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:785\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    784\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    790\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "from vllm import LLM\n",
    "\n",
    "# Sample prompts.\n",
    "text_1 = \"What is the capital of France?\"\n",
    "texts_2 = [\n",
    "    \"The capital of Brazil is Brasilia.\", \"The capital of France is Paris.\"\n",
    "]\n",
    "\n",
    "# Create an LLM.\n",
    "# You should pass task=\"score\" for cross-encoder models\n",
    "model = LLM(\n",
    "    model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    task=\"score\",\n",
    "    enforce_eager=True,\n",
    ")\n",
    "\n",
    "# Generate scores. The output is a list of ScoringRequestOutputs.\n",
    "outputs = model.score(text_1, texts_2)\n",
    "\n",
    "# Print the outputs.\n",
    "for text_2, output in zip(texts_2, outputs):\n",
    "    score = output.outputs.score\n",
    "    print(f\"Pair: {[text_1, text_2]!r} | Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c81acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n",
    "# ruff: noqa: E501\n",
    "# %pip install vllm\n",
    "from vllm import LLM\n",
    "\n",
    "model_name = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# What is the difference between the official original version and one\n",
    "# that has been converted into a sequence classification model?\n",
    "# Qwen3-Reranker is a language model that doing reranker by using the\n",
    "# logits of \"no\" and \"yes\" tokens.\n",
    "# It needs to computing 151669 tokens logits, making this method extremely\n",
    "# inefficient, not to mention incompatible with the vllm score API.\n",
    "# A method for converting the original model into a sequence classification\n",
    "# model was proposed. Seeï¼šhttps://huggingface.co/Qwen/Qwen3-Reranker-0.6B/discussions/3\n",
    "# Models converted offline using this method can not only be more efficient\n",
    "# and support the vllm score API, but also make the init parameters more\n",
    "# concise, for example.\n",
    "# model = LLM(model=\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\", task=\"score\")\n",
    "\n",
    "# If you want to load the official original version, the init parameters are\n",
    "# as follows.\n",
    "\n",
    "\n",
    "def get_model() -> LLM:\n",
    "    \"\"\"Initializes and returns the LLM model for Qwen3-Reranker.\"\"\"\n",
    "    return LLM(\n",
    "        model=model_name,\n",
    "        task=\"score\",\n",
    "        hf_overrides={\n",
    "            \"architectures\": [\"Qwen3ForSequenceClassification\"],\n",
    "            \"classifier_from_token\": [\"no\", \"yes\"],\n",
    "            \"is_original_qwen3_reranker\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# Why do we need hf_overrides for the official original version:\n",
    "# vllm converts it to Qwen3ForSequenceClassification when loaded for\n",
    "# better performance.\n",
    "# - Firstly, we need using `\"architectures\": [\"Qwen3ForSequenceClassification\"],`\n",
    "# to manually route to Qwen3ForSequenceClassification.\n",
    "# - Then, we will extract the vector corresponding to classifier_from_token\n",
    "# from lm_head using `\"classifier_from_token\": [\"no\", \"yes\"]`.\n",
    "# - Third, we will convert these two vectors into one vector.  The use of\n",
    "# conversion logic is controlled by `using \"is_original_qwen3_reranker\": True`.\n",
    "\n",
    "# Please use the query_template and document_template to format the query and\n",
    "# document for better reranker results.\n",
    "\n",
    "prefix = '<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n<|im_start|>user\\n'\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "query_template = \"{prefix}<Instruct>: {instruction}\\n<Query>: {query}\\n\"\n",
    "document_template = \"<Document>: {doc}{suffix}\"\n",
    "\n",
    "\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    query_template.format(prefix=prefix, instruction=instruction, query=query)\n",
    "    for query in queries\n",
    "]\n",
    "documents = [document_template.format(doc=doc, suffix=suffix) for doc in documents]\n",
    "\n",
    "# model = get_model()\n",
    "model = LLM(model=\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\", task=\"score\",\n",
    "max_model_len=512,\n",
    "gpu_memory_utilization = 0.1,\n",
    ")\n",
    "# outputs = model.score(queries, documents)\n",
    "\n",
    "# print(\"-\" * 30)\n",
    "# print([output.outputs.score for output in outputs])\n",
    "# print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "    \"It's very dark and high pressure\",\n",
    "    \"The bottom if mariana trench is very interesting\"\n",
    "]\n",
    "outputs = model.score(\"What is in the mariana trench\", documents)\n",
    "scores = [output.outputs.score for output in outputs]\n",
    "idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0b848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [output.outputs.score for output in outputs]\n",
    "idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b553d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever import retrieve\n",
    "out = retrieve(\"How far is the sun from earth\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92980e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b54d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3b5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
