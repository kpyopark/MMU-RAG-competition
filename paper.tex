% NeurIPS 2025 MMU-RAG Competition Paper
\documentclass{article}

\usepackage[final]{neurips_2025}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}

% It is recommended to load hyperref last to avoid conflicts
\usepackage{hyperref}

\title{Test-Time Diffusion Deep Researcher for MMU-RAG}

\author{
  Dmitrii Magas \\
  Independent Scientist \\
  Team TTT-DR \\
  \texttt{dmitrii@eamag.me}
}

\begin{document}

\maketitle

\begin{abstract}
We present our implementation of Test-Time Diffusion Deep Researcher (TTD-DR) for the MMU-RAG text-to-text track. Our system conceptualizes research report generation as a diffusion process, where an initial draft is iteratively refined through retrieval-augmented denoising. The framework combines two key mechanisms: (1) report-level refinement via denoising with retrieval, and (2) component-wise optimization via self-evolution. By maintaining a dynamic draft that guides the research direction, our system achieves more coherent information integration while reducing information loss during multi-hop search. We demonstrate the effectiveness of this approach on the MMU-RAG benchmark, which requires intensive search and complex reasoning over web-scale corpora.
\end{abstract}

\section{Introduction}

The MMU-RAG competition challenges participants to build RAG systems that handle real-world user queries requiring extensive search and reasoning. Traditional RAG approaches often struggle with complex queries that demand multiple rounds of information gathering and synthesis. Inspired by human research patterns~\cite{flower1981cognitive}, we implement the Test-Time Diffusion Deep Researcher (TTD-DR)~\cite{han2025deepresearcher}, which models research report generation as an iterative diffusion process.

Our key insight is that effective research involves maintaining an evolving understanding (draft) that guides subsequent searches, rather than collecting information linearly and synthesizing only at the end. This draft-centric approach enables more timely integration of discovered information and reduces the context loss common in long agentic trajectories.

\section{System Architecture}

\subsection{Backbone Deep Research Agent}

Our system operates in three main stages:

\textbf{Stage 1: Research Plan Generation.} Given a user query, we generate a structured research plan outlining key areas to investigate using self-evolution (see below). This plan serves as a high-level scaffold for the entire research process.

\textbf{Stage 2: Iterative Search and Synthesis.} This stage contains two sub-agents in a loop workflow:
\begin{itemize}
    \item \textit{Search Question Generation}: Formulates targeted search queries based on the research plan, user query, and previous search context
    \item \textit{Answer Synthesis}: Retrieves documents via FineWeb Search API, applies intelligent chunking and reranking, then synthesizes answers using a RAG-style approach
\end{itemize}

\textbf{Stage 3: Final Report Generation.} Synthesizes all gathered information (plan and question-answer pairs) into a comprehensive final report.

\subsection{Component-wise Self-Evolution}

To enhance each stage's output quality, we apply a self-evolutionary algorithm inspired by~\cite{lee2025evolving}. For each component (plan, question, answer):

\begin{enumerate}
    \item Generate multiple diverse initial variants
    \item Apply environmental feedback via LLM-as-a-judge for helpfulness and comprehensiveness
    \item Revise variants based on feedback scores and critiques
    \item Merge revised variants into a single high-quality output
\end{enumerate}

This process encourages exploration of diverse information and maintains high-quality context throughout the workflow.

\subsection{Report-level Denoising with Retrieval}

Our core innovation is treating report generation as a diffusion process. We initialize an early draft from the LLM's internal knowledge, then iteratively refine it:

\begin{enumerate}
    \item Generate initial "noisy" draft based on user query
    \item Feed current draft to Stage 2a to generate next search query
    \item Retrieve and synthesize answer (Stage 2b)
    \item Revise draft by integrating new information
    \item Repeat until sufficient information is gathered
\end{enumerate}

This continuous feedback loop ensures the evolving draft guides search direction while retrieved information progressively "denoises" the report. The synergy between self-evolution (providing quality context) and diffusion (maintaining coherence) is critical for performance.

\section{Implementation Details}

\textbf{Infrastructure.} We deploy our system as a Docker container with vLLM for efficient inference. The system exposes two endpoints:
\begin{itemize}
    \item \texttt{/evaluate}: Static evaluation endpoint accepting \texttt{\{query, iid\}} and returning \texttt{\{query\_id, generated\_response\}}
    \item \texttt{/run}: Streaming SSE endpoint emitting updates with \texttt{intermediate\_steps}, \texttt{final\_report}, \texttt{citations}, and \texttt{complete} flags
\end{itemize}

\textbf{Retrieval Pipeline.} Our retrieval system implements a sophisticated three-stage approach:
\begin{enumerate}
    \item \textit{Initial Retrieval}: Query FineWeb Search API to retrieve top-K documents (K=50)
    \item \textit{Intelligent Chunking}: Apply custom sentence-level chunking with:
    \begin{itemize}
        \item Maximum 500 tokens per chunk
        \item 50-token overlap between chunks
        \item Minimum 500 tokens to avoid fragmentary chunks
        \item Metadata preservation (doc\_id, URL, char\_range)
    \end{itemize}
    \item \textit{Neural Reranking}: Deploy Qwen3-Reranker-0.6B via vLLM in scoring mode to rerank chunks based on query relevance, returning top-K reranked chunks
\end{enumerate}

This reranking step is crucial for filtering the most relevant information from retrieved documents before synthesis.

\textbf{Generation Models.} We use a dual-model strategy:
\begin{itemize}
    \item \textit{Local Inference}: Qwen3-4B-Instruct-2507 served via vLLM (max 16K context, 75\% GPU utilization) for fast, local generation
    \item \textit{Fallback}: OpenRouter API with configurable models (default: tongyi-deepresearch-30b-a3b:free) for systems without GPU
\end{itemize}

\textbf{Optimization Strategy.} We set maximum denoising iterations to 3 (reduced from 20 in original paper for efficiency). For self-evolution we set all parameters conservatively to not exceed latency budgets using local GPU inference:
\begin{itemize}
    \item Generate N=1 variant for plans and reports
    \item Generate M=1 variant for search queries
    \item Apply K=1 evolution step across components
\end{itemize}

\section{Results and Discussion}

Our implementation focuses on the text-to-text track of MMU-RAG. Key technical innovations include:

\textbf{Neural Reranking for Quality.} Using Qwen3-Reranker-0.6B in vLLM's scoring mode enables efficient, GPU-accelerated reranking of retrieved chunks. This is particularly valuable given FineWeb's large result sets - we retrieve 50 documents, chunk them into potentially hundreds of segments, then rerank to select the top-K=20 most relevant chunks for synthesis.

\textbf{Draft-Centric Information Flow.} Unlike traditional RAG systems that collect all information before generation, our evolving draft:
\begin{itemize}
    \item Guides targeted search query formulation at each iteration
    \item Incorporates findings progressively, reducing context window pressure
    \item Maintains coherence across multi-hop reasoning chains
    \item Enables early stopping when sufficient information is gathered
\end{itemize}

\textbf{Practical Trade-offs.} We reduced maximum iterations from 20 (original paper) to 3 for competition constraints, balancing thoroughness with latency. Similarly, self-evolution uses minimal variants (1-2) rather than the paper's 3-5, prioritizing deployment efficiency.

\textbf{Strengths:}
\begin{itemize}
    \item Efficient GPU utilization via vLLM for both generation and reranking
    \item Modular chunking pipeline handles diverse document structures
    \item Graceful fallback to OpenRouter for non-GPU environments
    \item Clear separation of static (\texttt{/evaluate}) and streaming (\texttt{/run}) evaluation modes
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Conservative iteration limits may miss deeper insights for complex queries
    \item Single retrieval source (FineWeb); no ensemble or browsing tools
    \item Self-evolution limited to minimize latency overhead
    \item No learned components; fully reliant on prompting and in-context learning
\end{itemize}

\section{Conclusion}

We present a practical implementation of Test-Time Diffusion Deep Researcher for the MMU-RAG competition. Our system makes several key technical contributions: (1) efficient neural reranking via Qwen3-Reranker-0.6B in vLLM, (2) intelligent chunking that preserves document structure while optimizing for context windows, and (3) a draft-centric diffusion process that guides multi-hop search. By balancing the paper's ambitious algorithms with competition constraints, we demonstrate that diffusion-based research agents can be deployed efficiently. Future work includes exploring learned reranking models, integrating browsing tools, and adaptive iteration budgets based on query complexity.

\section*{Acknowledgments}

We thank the MMU-RAG organizers for creating this benchmark, the TTD-DR authors for the foundational research, and the vLLM team for their efficient inference framework.

% The command below is for BibTeX and causes an error with a manual bibliography.
% \bibliographystyle{plain} 

\begin{thebibliography}{9}

\bibitem{flower1981cognitive}
L. Flower and J.R. Hayes,
\textit{A cognitive process theory of writing},
College Composition and Communication, 32(4):365--387, 1981.

\bibitem{han2025deepresearcher}
R. Han, Y. Chen, Z. CuiZhu, et al.,
\textit{Deep Researcher with Test-Time Diffusion},
arXiv:2507.16075, 2025.

\bibitem{lee2025evolving}
K.-H. Lee, I. Fischer, Y.-H. Wu, et al.,
\textit{Evolving deeper LLM thinking},
arXiv:2501.09891, 2025.

\end{thebibliography}

\end{document}